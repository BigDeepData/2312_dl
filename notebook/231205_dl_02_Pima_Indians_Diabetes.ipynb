{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jW8dVZL7HogW"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리를 임포트합니다.\n",
        "import pandas as pd  # 데이터 처리를 위한 라이브러리\n",
        "from sklearn.model_selection import train_test_split  # 데이터 분할을 위한 함수\n",
        "from sklearn.preprocessing import StandardScaler  # 데이터 정규화를 위한 클래스\n",
        "from keras.models import Sequential  # Keras에서 순차적 모델 구성을 위한 클래스\n",
        "from keras.layers import Dense  # Keras에서 Dense(밀집) 레이어를 사용하기 위한 클래스"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 불러오기\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/BigDeepData/2312_dl/main/data/diabetes.csv')\n",
        "\n",
        "# 데이터셋의 특성(독립 변수)과 라벨(종속 변수)을 분리합니다.\n",
        "X = data.iloc[:, 0:8]  # 첫 8개 컬럼은 특성을 나타냅니다.\n",
        "y = data.iloc[:, 8]    # 마지막 컬럼은 라벨(당뇨병 발병 여부)을 나타냅니다.\n",
        "\n",
        "# 데이터 정규화: 각 특성의 범위를 표준화합니다 (평균 0, 표준편차 1).\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 데이터를 학습 데이터와 테스트 데이터로 분리합니다.\n",
        "# 일반적으로 80%는 학습용, 20%는 테스트용으로 사용합니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "2YKZ0JcVH3gr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras를 이용한 딥러닝 모델 정의\n",
        "model = Sequential()\n",
        "# Sequential 모델은 Keras에서 층(layer)을 순차적으로 쌓아가는 모델입니다.\n",
        "\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "# 첫 번째 층(입력층)을 추가합니다.\n",
        "# 'Dense'는 완전 연결 레이어를 의미합니다.\n",
        "# 이 층에는 12개의 노드(뉴런)이 있으며, 입력 차원(input_dim)은 8입니다 (데이터셋의 특성 수).\n",
        "# 'relu'는 활성화 함수로, 각 노드의 출력을 결정합니다.\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "# 두 번째 층(은닉층)을 추가합니다.\n",
        "# 이 층에는 8개의 노드가 있으며, 'relu' 활성화 함수를 사용합니다.\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# 세 번째 층(출력층)을 추가합니다.\n",
        "# 이 층에는 1개의 노드가 있으며, 'sigmoid' 활성화 함수를 사용합니다.\n",
        "# 'sigmoid' 함수는 출력값을 0과 1 사이로 제한하여, 이진 분류 문제에 적합합니다.\n",
        "\n",
        "model.summary()\n",
        "# 모델의 요약 정보를 출력합니다. 이를 통해 각 층의 구성, 파라미터 수 등을 확인할 수 있습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LUmWWJRIQ5s",
        "outputId": "b0d0183f-d049-460f-bc4d-87f8cdf694bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 12)                108       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 221 (884.00 Byte)\n",
            "Trainable params: 221 (884.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',  # 손실 함수(loss function) 설정\n",
        "    optimizer='adam',            # 최적화 알고리즘(optimizer) 설정\n",
        "    metrics=['accuracy'])        # 평가 기준(metrics) 설정\n",
        "\n",
        "# 'binary_crossentropy' 손실 함수는 이진 분류 문제에 적합한 손실 함수입니다.\n",
        "# 이 함수는 실제 라벨과 예측된 확률 간의 차이를 측정합니다.\n",
        "\n",
        "# 'adam' 최적화 알고리즘은 효율적인 경사 하강법의 한 형태로,\n",
        "# 학습 과정에서 자동으로 학습률을 조정합니다.\n",
        "\n",
        "# 'metrics'에 'accuracy'를 지정함으로써,\n",
        "# 훈련 및 검증 과정에서 모델의 정확도를 평가할 수 있습니다."
      ],
      "metadata": {
        "id": "N31PbIx9IeTw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "model.fit(\n",
        "    X_train, y_train,   # 학습 데이터와 라벨\n",
        "    epochs=150,         # 에폭(epoch) 수 설정\n",
        "    batch_size=10)      # 배치 크기(batch size) 설정\n",
        "\n",
        "# 'X_train'과 'y_train'은 모델을 훈련시키기 위한 학습 데이터와 해당 라벨입니다.\n",
        "\n",
        "# 'epochs=150'은 모델이 전체 데이터셋에 대해 학습을 150회 반복한다는 것을 의미합니다.\n",
        "# 에폭 수는 모델이 학습 데이터를 얼마나 많이 반복하여 볼 것인지 결정합니다.\n",
        "\n",
        "# 'batch_size=10'은 한 번의 에폭에서 10개의 샘플을 처리한 후 가중치를 업데이트한다는 의미입니다.\n",
        "# 배치 크기는 메모리 사용량과 학습 속도에 영향을 미치며, 너무 크거나 작으면 학습 성능에 부정적인 영향을 미칠 수 있습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q4QrFf9IWgr",
        "outputId": "cdebc844-c01e-49b2-feeb-6368527efa53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 0.7265 - accuracy: 0.6254\n",
            "Epoch 2/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.6596\n",
            "Epoch 3/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.6661\n",
            "Epoch 4/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.6710\n",
            "Epoch 5/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.6840\n",
            "Epoch 6/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7003\n",
            "Epoch 7/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7541\n",
            "Epoch 8/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.7736\n",
            "Epoch 9/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.7801\n",
            "Epoch 10/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.7704\n",
            "Epoch 11/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.7769\n",
            "Epoch 12/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.7866\n",
            "Epoch 13/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4652 - accuracy: 0.7769\n",
            "Epoch 14/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4620 - accuracy: 0.7818\n",
            "Epoch 15/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.7834\n",
            "Epoch 16/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.7883\n",
            "Epoch 17/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.7866\n",
            "Epoch 18/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.7866\n",
            "Epoch 19/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4532 - accuracy: 0.7932\n",
            "Epoch 20/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.7915\n",
            "Epoch 21/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4496 - accuracy: 0.7997\n",
            "Epoch 22/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.7932\n",
            "Epoch 23/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.7997\n",
            "Epoch 24/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4453 - accuracy: 0.8046\n",
            "Epoch 25/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4447 - accuracy: 0.7964\n",
            "Epoch 26/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.7964\n",
            "Epoch 27/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.8013\n",
            "Epoch 28/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.7948\n",
            "Epoch 29/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.7997\n",
            "Epoch 30/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.7948\n",
            "Epoch 31/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8013\n",
            "Epoch 32/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.7997\n",
            "Epoch 33/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.8062\n",
            "Epoch 34/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.8013\n",
            "Epoch 35/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.8078\n",
            "Epoch 36/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8078\n",
            "Epoch 37/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4302 - accuracy: 0.8062\n",
            "Epoch 38/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.8062\n",
            "Epoch 39/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8062\n",
            "Epoch 40/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8062\n",
            "Epoch 41/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8078\n",
            "Epoch 42/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8127\n",
            "Epoch 43/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8046\n",
            "Epoch 44/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8078\n",
            "Epoch 45/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8094\n",
            "Epoch 46/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8094\n",
            "Epoch 47/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4209 - accuracy: 0.8062\n",
            "Epoch 48/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.8029\n",
            "Epoch 49/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8046\n",
            "Epoch 50/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8111\n",
            "Epoch 51/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8143\n",
            "Epoch 52/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8094\n",
            "Epoch 53/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8094\n",
            "Epoch 54/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8029\n",
            "Epoch 55/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8111\n",
            "Epoch 56/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8143\n",
            "Epoch 57/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8127\n",
            "Epoch 58/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8176\n",
            "Epoch 59/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4089 - accuracy: 0.8143\n",
            "Epoch 60/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8160\n",
            "Epoch 61/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8192\n",
            "Epoch 62/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8160\n",
            "Epoch 63/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.8257\n",
            "Epoch 64/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8127\n",
            "Epoch 65/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8241\n",
            "Epoch 66/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8176\n",
            "Epoch 67/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8192\n",
            "Epoch 68/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8192\n",
            "Epoch 69/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8241\n",
            "Epoch 70/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8208\n",
            "Epoch 71/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8241\n",
            "Epoch 72/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8127\n",
            "Epoch 73/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8225\n",
            "Epoch 74/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8176\n",
            "Epoch 75/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8160\n",
            "Epoch 76/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8160\n",
            "Epoch 77/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8208\n",
            "Epoch 78/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.8257\n",
            "Epoch 79/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8241\n",
            "Epoch 80/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8208\n",
            "Epoch 81/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8225\n",
            "Epoch 82/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8274\n",
            "Epoch 83/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8274\n",
            "Epoch 84/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8274\n",
            "Epoch 85/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8257\n",
            "Epoch 86/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8290\n",
            "Epoch 87/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8290\n",
            "Epoch 88/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8306\n",
            "Epoch 89/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8257\n",
            "Epoch 90/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8241\n",
            "Epoch 91/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8274\n",
            "Epoch 92/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8355\n",
            "Epoch 93/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8322\n",
            "Epoch 94/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8306\n",
            "Epoch 95/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8355\n",
            "Epoch 96/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8371\n",
            "Epoch 97/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8306\n",
            "Epoch 98/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8371\n",
            "Epoch 99/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.8355\n",
            "Epoch 100/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8322\n",
            "Epoch 101/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8290\n",
            "Epoch 102/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8322\n",
            "Epoch 103/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8355\n",
            "Epoch 104/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3801 - accuracy: 0.8371\n",
            "Epoch 105/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8355\n",
            "Epoch 106/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8339\n",
            "Epoch 107/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8355\n",
            "Epoch 108/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8339\n",
            "Epoch 109/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8388\n",
            "Epoch 110/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8420\n",
            "Epoch 111/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8339\n",
            "Epoch 112/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3767 - accuracy: 0.8404\n",
            "Epoch 113/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8371\n",
            "Epoch 114/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8371\n",
            "Epoch 115/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8355\n",
            "Epoch 116/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8404\n",
            "Epoch 117/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8322\n",
            "Epoch 118/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8388\n",
            "Epoch 119/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3733 - accuracy: 0.8339\n",
            "Epoch 120/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8404\n",
            "Epoch 121/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8355\n",
            "Epoch 122/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8339\n",
            "Epoch 123/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8355\n",
            "Epoch 124/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8453\n",
            "Epoch 125/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8388\n",
            "Epoch 126/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8371\n",
            "Epoch 127/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8388\n",
            "Epoch 128/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8388\n",
            "Epoch 129/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3712 - accuracy: 0.8371\n",
            "Epoch 130/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8355\n",
            "Epoch 131/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8322\n",
            "Epoch 132/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8436\n",
            "Epoch 133/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8388\n",
            "Epoch 134/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8371\n",
            "Epoch 135/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8404\n",
            "Epoch 136/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8371\n",
            "Epoch 137/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8404\n",
            "Epoch 138/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3640 - accuracy: 0.8371\n",
            "Epoch 139/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3649 - accuracy: 0.8388\n",
            "Epoch 140/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3641 - accuracy: 0.8436\n",
            "Epoch 141/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8436\n",
            "Epoch 142/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8388\n",
            "Epoch 143/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.8453\n",
            "Epoch 144/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8436\n",
            "Epoch 145/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.8469\n",
            "Epoch 146/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8420\n",
            "Epoch 147/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8420\n",
            "Epoch 148/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8420\n",
            "Epoch 149/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8420\n",
            "Epoch 150/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.8469\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ca7318ee0e0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "# 'model.evaluate' 함수를 사용하여 테스트 데이터셋에 대한 모델의 성능을 평가합니다.\n",
        "# 이 함수는 손실값(loss)과 metrics에 지정된 성능 지표(여기서는 정확도)를 반환합니다.\n",
        "\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "# 계산된 정확도를 백분율로 출력합니다.\n",
        "\n",
        "# 'X_test'와 'y_test'는 테스트 데이터셋과 해당 라벨입니다.\n",
        "# 이 데이터셋은 모델의 학습 과정에서 사용되지 않았으며,\n",
        "# 학습된 모델의 일반화 성능을 평가하는 데 사용됩니다.\n",
        "\n",
        "# '_' (언더스코어)는 이 함수가 반환하는 첫 번째 값인 손실값을 무시하겠다는 의미입니다.\n",
        "# 여기서 우리는 정확도만 관심이 있기 때문에 손실값은 무시합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z6xad5IIjYy",
        "outputId": "f3fbbf33-1ce8-4df2-c689-421e4a73d44b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7403\n",
            "Accuracy: 74.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 데이터에 대한 예측\n",
        "predictions = model.predict(X_test)\n",
        "# 'model.predict' 함수를 사용하여 테스트 데이터셋에 대한 예측을 수행합니다.\n",
        "# 이 함수는 각 샘플에 대해 모델이 예측한 출력(여기서는 당뇨병 발병 확률)을 반환합니다.\n",
        "\n",
        "rounded = [round(x[0]) for x in predictions]\n",
        "# 모델의 출력은 확률 형태로 나타나므로, 이를 이진 분류 결과(0 또는 1)로 변환합니다.\n",
        "# 여기서는 각 예측값을 반올림하여 0 또는 1의 값으로 변환합니다.\n",
        "\n",
        "rounded\n",
        "# 반올림된 예측 결과를 출력합니다.\n",
        "\n",
        "# 'X_test'는 테스트 데이터셋을 나타냅니다.\n",
        "# 이 데이터셋은 모델이 이전에 본 적 없는 데이터로,\n",
        "# 모델이 얼마나 잘 일반화되었는지를 평가하는 데 사용됩니다."
      ],
      "metadata": {
        "id": "o9z2RqGzIZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model.save('pima_indians_diabetes_model.h5')\n",
        "# 'model.save' 함수를 사용하여 전체 모델을 하나의 HDF5 파일로 저장합니다.\n",
        "# 이 파일에는 모델의 구조, 가중치 값, 학습 설정(컴파일에 사용된 설정), 옵티마이저 상태 등이 포함됩니다.\n",
        "\n",
        "# 'pima_indians_diabetes_model.h5'는 저장할 파일의 이름입니다.\n",
        "# '.h5' 확장자는 HDF5 포맷을 나타냅니다,\n",
        "# 이 포맷은 대용량 데이터를 저장하기에 적합하며, 모델을 효율적으로 저장하고 로드할 수 있게 해줍니다.\n",
        "\n",
        "# 이렇게 저장된 모델은 나중에 다시 로드하여 사용할 수 있으며,\n",
        "# 추가 학습이나 다른 데이터셋에 대한 예측 등에 활용할 수 있습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QbjNZCBIn4h",
        "outputId": "f7fa21f6-1eb2-42c7-ab4f-7ebd2e21e38b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# 저장된 모델 로드\n",
        "loaded_model = load_model('pima_indians_diabetes_model.h5')\n",
        "# 'load_model' 함수를 사용하여 저장된 HDF5 파일로부터 모델을 로드합니다.\n",
        "# 'pima_indians_diabetes_model.h5' 파일에 저장된 모델의 구조, 가중치, 컴파일 정보 등이 로드됩니다.\n",
        "\n",
        "_, accuracy = loaded_model.evaluate(X_test, y_test)\n",
        "# 로드된 모델을 사용하여 테스트 데이터셋에 대해 평가합니다.\n",
        "# 'evaluate' 함수는 모델의 손실값과 정확도를 반환합니다.\n",
        "# 여기서는 정확도만 필요하기 때문에 손실값은 무시합니다('_' 사용).\n",
        "\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "# 계산된 정확도를 백분율로 출력합니다.\n",
        "\n",
        "# 'X_test'와 'y_test'는 이전에 분리된 테스트 데이터셋과 해당 라벨을 나타냅니다.\n",
        "# 이 데이터셋은 모델의 일반화 성능을 평가하는 데 사용됩니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV6Urg2rIrjs",
        "outputId": "b07f37ec-88db-4393-c866-1cebcea83eda"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7403\n",
            "Accuracy: 74.03%\n"
          ]
        }
      ]
    }
  ]
}