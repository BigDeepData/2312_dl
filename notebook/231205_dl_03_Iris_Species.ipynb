{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SqNV1F8VEUVs"
      },
      "outputs": [],
      "source": [
        "# pandas를 불러옵니다. pandas는 데이터를 다루는데 유용한 라이브러리입니다.\n",
        "import pandas as pd\n",
        "\n",
        "# scikit-learn에서 train_test_split을 불러옵니다.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# scikit-learn의 StandardScaler와 OneHotEncoder를 불러옵니다.\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Keras에서 Sequential, Dense, EarlyStopping을 불러옵니다.\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋을 불러오기 위해 Pandas를 사용합니다. 데이터는 외부 URL에서 가져옵니다.\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/BigDeepData/2312_dl/main/data/Iris.csv')\n",
        "\n",
        "# 데이터셋의 특성과 라벨을 분리합니다.\n",
        "# 특성 데이터는 첫 번째부터 네 번째 컬럼을 사용합니다.\n",
        "X = data.iloc[:, 1:5]  # 1부터 4까지의 컬럼 선택\n",
        "# 라벨 데이터는 다섯 번째 컬럼을 사용합니다.\n",
        "y = data.iloc[:, 5]    # 5번째 컬럼 선택 (라벨)\n",
        "\n",
        "# 라벨을 원-핫 인코딩합니다.\n",
        "encoder = OneHotEncoder()\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1)).toarray()\n",
        "\n",
        "# 데이터를 정규화합니다. StandardScaler를 사용하여 평균이 0, 표준편차가 1이 되도록 스케일링합니다.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리합니다. test_size는 테스트 데이터의 비율을 나타냅니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2)"
      ],
      "metadata": {
        "id": "f2E0fXMoEfEV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential 모델을 생성합니다. Sequential 모델은 레이어를 순차적으로 쌓을 수 있는 모델입니다.\n",
        "model = Sequential()\n",
        "\n",
        "# 입력층을 추가합니다. input_dim은 입력 특성의 개수를 나타냅니다 (4개의 특성).\n",
        "# 활성화 함수로 'relu'를 사용합니다.\n",
        "model.add(Dense(10, input_dim=4, activation='relu'))  # 입력층\n",
        "\n",
        "# 은닉층을 추가합니다. 10개의 뉴런을 가진 은닉층을 만듭니다.\n",
        "# 활성화 함수로 'relu'를 사용합니다.\n",
        "model.add(Dense(10, activation='relu'))               # 은닉층\n",
        "\n",
        "# 출력층을 추가합니다. 출력층은 클래스의 개수와 같은 뉴런 수를 가지며,\n",
        "# 다중 클래스 분류 문제이므로 활성화 함수로 'softmax'를 사용합니다.\n",
        "model.add(Dense(3, activation='softmax'))             # 출력층\n",
        "\n",
        "# 모델의 구조를 요약하여 출력합니다.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ipd8COzEn25",
        "outputId": "0ef2d9a1-8613-4fcd-893d-63e2ebd5bf00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                50        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 193 (772.00 Byte)\n",
            "Trainable params: 193 (772.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 컴파일합니다. 컴파일 단계에서는 모델의 학습 방법을 설정합니다.\n",
        "\n",
        "# 손실 함수(loss function)를 'categorical_crossentropy'로 설정합니다.\n",
        "# 이것은 다중 클래스 분류 문제에 적합한 손실 함수입니다.\n",
        "# 다중 클래스 분류에서는 각 클래스에 대한 확률 분포의 차이를 최소화하는 것이 목표입니다.\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "\n",
        "              # 최적화 알고리즘으로 'adam'을 사용합니다.\n",
        "              # 'adam'은 효과적으로 경사 하강법을 수행하는 알고리즘 중 하나입니다.\n",
        "              optimizer='adam',\n",
        "\n",
        "              # 모델을 평가할 때 사용할 지표를 설정합니다.\n",
        "              # 'accuracy'는 정확도를 나타내며, 올바르게 분류된 샘플의 비율입니다.\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lABYhppfE15k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 조기 종료 콜백(EarlyStopping)을 설정합니다.\n",
        "\n",
        "# monitor 매개변수는 모니터링할 지표를 선택합니다.\n",
        "# 여기서 'val_loss'를 선택하면 검증 데이터셋의 손실 함수 값이 모니터링됩니다.\n",
        "early_stopping = EarlyStopping(\n",
        "\n",
        "    # 모니터링할 지표로 'val_loss'를 선택합니다.\n",
        "    monitor='val_loss',\n",
        "\n",
        "    # 'patience' 매개변수는 조기 종료를 얼마나 오랫동안 지켜볼지를 나타냅니다.\n",
        "    # 예를 들어, 'patience=10'은 10 에포크 동안 검증 손실이 개선되지 않으면 학습을 조기 종료합니다.\n",
        "    patience=10\n",
        ")"
      ],
      "metadata": {
        "id": "XogknctQE2zl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 학습합니다.\n",
        "\n",
        "# X_train: 학습 데이터의 입력 특성\n",
        "# y_train: 학습 데이터의 라벨 (원-핫 인코딩된 형태)\n",
        "# epochs: 학습을 몇 번 반복할지를 나타냅니다. 여기서는 200번을 설정했습니다.\n",
        "# batch_size: 한 번에 학습할 샘플의 개수를 나타냅니다. 여기서는 10으로 설정했습니다.\n",
        "# validation_split: 학습 데이터 중 일부를 검증(validation) 데이터로 사용할 비율을 나타냅니다.\n",
        "# 여기서는 0.2로 설정했으므로 학습 데이터의 20%를 검증 데이터로 사용합니다.\n",
        "# callbacks: 모델 학습 중에 사용할 콜백 함수를 설정합니다. 여기서는 조기 종료 콜백을 사용합니다.\n",
        "\n",
        "model.fit(\n",
        "    X_train,                    # 학습 데이터의 입력 특성\n",
        "    y_train,                    # 학습 데이터의 라벨\n",
        "    epochs=200,                 # 학습 반복 횟수\n",
        "    batch_size=10,              # 한 번에 학습할 샘플 개수\n",
        "    validation_split=0.2,       # 검증 데이터의 비율\n",
        "    callbacks=[early_stopping]  # 조기 종료 콜백 사용\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOfPDnATE6L-",
        "outputId": "c827f07a-1eed-4b1d-c4fa-2b7eb997fb06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "10/10 [==============================] - 2s 27ms/step - loss: 1.1959 - accuracy: 0.4583 - val_loss: 1.2940 - val_accuracy: 0.4167\n",
            "Epoch 2/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.1309 - accuracy: 0.5938 - val_loss: 1.2353 - val_accuracy: 0.4583\n",
            "Epoch 3/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 1.0739 - accuracy: 0.6042 - val_loss: 1.1782 - val_accuracy: 0.5000\n",
            "Epoch 4/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.0174 - accuracy: 0.6250 - val_loss: 1.1266 - val_accuracy: 0.5417\n",
            "Epoch 5/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.9640 - accuracy: 0.6562 - val_loss: 1.0819 - val_accuracy: 0.5417\n",
            "Epoch 6/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.9103 - accuracy: 0.6667 - val_loss: 1.0368 - val_accuracy: 0.5417\n",
            "Epoch 7/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.8621 - accuracy: 0.6667 - val_loss: 0.9943 - val_accuracy: 0.5417\n",
            "Epoch 8/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.8136 - accuracy: 0.6771 - val_loss: 0.9562 - val_accuracy: 0.5417\n",
            "Epoch 9/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.7704 - accuracy: 0.6875 - val_loss: 0.9206 - val_accuracy: 0.5417\n",
            "Epoch 10/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.7285 - accuracy: 0.7083 - val_loss: 0.8884 - val_accuracy: 0.5417\n",
            "Epoch 11/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.7188 - val_loss: 0.8593 - val_accuracy: 0.5417\n",
            "Epoch 12/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.6547 - accuracy: 0.7188 - val_loss: 0.8337 - val_accuracy: 0.5417\n",
            "Epoch 13/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6238 - accuracy: 0.7292 - val_loss: 0.8073 - val_accuracy: 0.5417\n",
            "Epoch 14/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.5944 - accuracy: 0.7292 - val_loss: 0.7839 - val_accuracy: 0.5833\n",
            "Epoch 15/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.5685 - accuracy: 0.7396 - val_loss: 0.7619 - val_accuracy: 0.5833\n",
            "Epoch 16/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.5445 - accuracy: 0.7500 - val_loss: 0.7423 - val_accuracy: 0.6250\n",
            "Epoch 17/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.5229 - accuracy: 0.7500 - val_loss: 0.7203 - val_accuracy: 0.6250\n",
            "Epoch 18/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.7604 - val_loss: 0.7003 - val_accuracy: 0.6250\n",
            "Epoch 19/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.4830 - accuracy: 0.7708 - val_loss: 0.6847 - val_accuracy: 0.6250\n",
            "Epoch 20/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.4650 - accuracy: 0.7917 - val_loss: 0.6697 - val_accuracy: 0.6250\n",
            "Epoch 21/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.4489 - accuracy: 0.8021 - val_loss: 0.6528 - val_accuracy: 0.7083\n",
            "Epoch 22/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.4333 - accuracy: 0.8021 - val_loss: 0.6392 - val_accuracy: 0.7083\n",
            "Epoch 23/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.4183 - accuracy: 0.8125 - val_loss: 0.6269 - val_accuracy: 0.7083\n",
            "Epoch 24/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.4046 - accuracy: 0.8229 - val_loss: 0.6133 - val_accuracy: 0.7083\n",
            "Epoch 25/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3907 - accuracy: 0.8438 - val_loss: 0.5959 - val_accuracy: 0.7083\n",
            "Epoch 26/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3775 - accuracy: 0.8438 - val_loss: 0.5803 - val_accuracy: 0.7500\n",
            "Epoch 27/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3639 - accuracy: 0.8646 - val_loss: 0.5648 - val_accuracy: 0.7500\n",
            "Epoch 28/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3517 - accuracy: 0.8646 - val_loss: 0.5541 - val_accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3388 - accuracy: 0.8750 - val_loss: 0.5438 - val_accuracy: 0.7500\n",
            "Epoch 30/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8958 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
            "Epoch 31/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.3169 - accuracy: 0.8958 - val_loss: 0.5198 - val_accuracy: 0.7500\n",
            "Epoch 32/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3056 - accuracy: 0.9062 - val_loss: 0.5093 - val_accuracy: 0.7500\n",
            "Epoch 33/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.2954 - accuracy: 0.9062 - val_loss: 0.4994 - val_accuracy: 0.7500\n",
            "Epoch 34/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2855 - accuracy: 0.9167 - val_loss: 0.4896 - val_accuracy: 0.7500\n",
            "Epoch 35/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2759 - accuracy: 0.9271 - val_loss: 0.4799 - val_accuracy: 0.7500\n",
            "Epoch 36/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2666 - accuracy: 0.9271 - val_loss: 0.4698 - val_accuracy: 0.7500\n",
            "Epoch 37/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2577 - accuracy: 0.9271 - val_loss: 0.4630 - val_accuracy: 0.7917\n",
            "Epoch 38/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2481 - accuracy: 0.9271 - val_loss: 0.4520 - val_accuracy: 0.7917\n",
            "Epoch 39/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2396 - accuracy: 0.9375 - val_loss: 0.4405 - val_accuracy: 0.7917\n",
            "Epoch 40/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2311 - accuracy: 0.9375 - val_loss: 0.4301 - val_accuracy: 0.7917\n",
            "Epoch 41/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2222 - accuracy: 0.9375 - val_loss: 0.4236 - val_accuracy: 0.7917\n",
            "Epoch 42/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2138 - accuracy: 0.9479 - val_loss: 0.4172 - val_accuracy: 0.7917\n",
            "Epoch 43/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2054 - accuracy: 0.9479 - val_loss: 0.4101 - val_accuracy: 0.8333\n",
            "Epoch 44/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1976 - accuracy: 0.9479 - val_loss: 0.4073 - val_accuracy: 0.8333\n",
            "Epoch 45/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1898 - accuracy: 0.9583 - val_loss: 0.3964 - val_accuracy: 0.8333\n",
            "Epoch 46/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1822 - accuracy: 0.9688 - val_loss: 0.3891 - val_accuracy: 0.8333\n",
            "Epoch 47/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9688 - val_loss: 0.3825 - val_accuracy: 0.8333\n",
            "Epoch 48/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.9688 - val_loss: 0.3784 - val_accuracy: 0.8333\n",
            "Epoch 49/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1624 - accuracy: 0.9688 - val_loss: 0.3795 - val_accuracy: 0.8333\n",
            "Epoch 50/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1553 - accuracy: 0.9688 - val_loss: 0.3696 - val_accuracy: 0.8333\n",
            "Epoch 51/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1494 - accuracy: 0.9688 - val_loss: 0.3649 - val_accuracy: 0.8333\n",
            "Epoch 52/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1440 - accuracy: 0.9688 - val_loss: 0.3622 - val_accuracy: 0.8333\n",
            "Epoch 53/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9688 - val_loss: 0.3591 - val_accuracy: 0.8333\n",
            "Epoch 54/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1332 - accuracy: 0.9688 - val_loss: 0.3517 - val_accuracy: 0.8333\n",
            "Epoch 55/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9688 - val_loss: 0.3400 - val_accuracy: 0.7917\n",
            "Epoch 56/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1244 - accuracy: 0.9688 - val_loss: 0.3436 - val_accuracy: 0.8333\n",
            "Epoch 57/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1196 - accuracy: 0.9688 - val_loss: 0.3356 - val_accuracy: 0.7917\n",
            "Epoch 58/200\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9792 - val_loss: 0.3342 - val_accuracy: 0.7917\n",
            "Epoch 59/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.1103 - accuracy: 0.9792 - val_loss: 0.3373 - val_accuracy: 0.8333\n",
            "Epoch 60/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1067 - accuracy: 0.9792 - val_loss: 0.3351 - val_accuracy: 0.8333\n",
            "Epoch 61/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1037 - accuracy: 0.9792 - val_loss: 0.3338 - val_accuracy: 0.8333\n",
            "Epoch 62/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0996 - accuracy: 0.9896 - val_loss: 0.3240 - val_accuracy: 0.7917\n",
            "Epoch 63/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0959 - accuracy: 0.9896 - val_loss: 0.3194 - val_accuracy: 0.7917\n",
            "Epoch 64/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0923 - accuracy: 0.9896 - val_loss: 0.3204 - val_accuracy: 0.7917\n",
            "Epoch 65/200\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0891 - accuracy: 0.9896 - val_loss: 0.3189 - val_accuracy: 0.7917\n",
            "Epoch 66/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0862 - accuracy: 0.9896 - val_loss: 0.3144 - val_accuracy: 0.7917\n",
            "Epoch 67/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0830 - accuracy: 0.9896 - val_loss: 0.3139 - val_accuracy: 0.7917\n",
            "Epoch 68/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0803 - accuracy: 0.9896 - val_loss: 0.3149 - val_accuracy: 0.7917\n",
            "Epoch 69/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0773 - accuracy: 0.9896 - val_loss: 0.3121 - val_accuracy: 0.7917\n",
            "Epoch 70/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0749 - accuracy: 0.9896 - val_loss: 0.3088 - val_accuracy: 0.7917\n",
            "Epoch 71/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0723 - accuracy: 0.9896 - val_loss: 0.3090 - val_accuracy: 0.7917\n",
            "Epoch 72/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0700 - accuracy: 0.9896 - val_loss: 0.3108 - val_accuracy: 0.7917\n",
            "Epoch 73/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0674 - accuracy: 0.9896 - val_loss: 0.3089 - val_accuracy: 0.7917\n",
            "Epoch 74/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0656 - accuracy: 0.9896 - val_loss: 0.3033 - val_accuracy: 0.7917\n",
            "Epoch 75/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0637 - accuracy: 0.9896 - val_loss: 0.2957 - val_accuracy: 0.7917\n",
            "Epoch 76/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0607 - accuracy: 0.9896 - val_loss: 0.3013 - val_accuracy: 0.7917\n",
            "Epoch 77/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0587 - accuracy: 0.9896 - val_loss: 0.3088 - val_accuracy: 0.7917\n",
            "Epoch 78/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0577 - accuracy: 0.9896 - val_loss: 0.3105 - val_accuracy: 0.7917\n",
            "Epoch 79/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0555 - accuracy: 0.9896 - val_loss: 0.3064 - val_accuracy: 0.7917\n",
            "Epoch 80/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0538 - accuracy: 0.9896 - val_loss: 0.3025 - val_accuracy: 0.7917\n",
            "Epoch 81/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0525 - accuracy: 0.9896 - val_loss: 0.3064 - val_accuracy: 0.7917\n",
            "Epoch 82/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9896 - val_loss: 0.3026 - val_accuracy: 0.7917\n",
            "Epoch 83/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9896 - val_loss: 0.3031 - val_accuracy: 0.7917\n",
            "Epoch 84/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0482 - accuracy: 0.9896 - val_loss: 0.2950 - val_accuracy: 0.7917\n",
            "Epoch 85/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0467 - accuracy: 0.9896 - val_loss: 0.3014 - val_accuracy: 0.7917\n",
            "Epoch 86/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0449 - accuracy: 0.9896 - val_loss: 0.2991 - val_accuracy: 0.7917\n",
            "Epoch 87/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0437 - accuracy: 0.9896 - val_loss: 0.2989 - val_accuracy: 0.7917\n",
            "Epoch 88/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0425 - accuracy: 0.9896 - val_loss: 0.2976 - val_accuracy: 0.7917\n",
            "Epoch 89/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0413 - accuracy: 0.9896 - val_loss: 0.2949 - val_accuracy: 0.7917\n",
            "Epoch 90/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0402 - accuracy: 0.9896 - val_loss: 0.2953 - val_accuracy: 0.7917\n",
            "Epoch 91/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0393 - accuracy: 0.9896 - val_loss: 0.2927 - val_accuracy: 0.7917\n",
            "Epoch 92/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0379 - accuracy: 0.9896 - val_loss: 0.2943 - val_accuracy: 0.7917\n",
            "Epoch 93/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0369 - accuracy: 0.9896 - val_loss: 0.2943 - val_accuracy: 0.7917\n",
            "Epoch 94/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.3031 - val_accuracy: 0.7917\n",
            "Epoch 95/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.7917\n",
            "Epoch 96/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.3107 - val_accuracy: 0.7917\n",
            "Epoch 97/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.3015 - val_accuracy: 0.7917\n",
            "Epoch 98/200\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.2941 - val_accuracy: 0.7917\n",
            "Epoch 99/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.2954 - val_accuracy: 0.7917\n",
            "Epoch 100/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 0.7917\n",
            "Epoch 101/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.3050 - val_accuracy: 0.7917\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792cd0d5d870>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 평가합니다.\n",
        "\n",
        "# model.evaluate() 함수를 사용하여 모델을 평가합니다.\n",
        "# X_test: 테스트 데이터의 입력 특성\n",
        "# y_test: 테스트 데이터의 실제 라벨 (원-핫 인코딩된 형태)\n",
        "\n",
        "# 모델의 평가 결과는 _, accuracy 변수에 저장됩니다.\n",
        "# 여기서 '_'는 무시할 값으로, 손실(loss)을 나타내는 값이지만 이 코드에서는 사용하지 않습니다.\n",
        "\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# 정확도를 출력합니다.\n",
        "# accuracy 변수에 저장된 값은 정확도를 나타냅니다.\n",
        "# 정확도는 올바르게 분류된 샘플의 비율을 나타냅니다.\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKaKBxY-E8FE",
        "outputId": "e128d1e4-a313-4678-c9cb-a3b9083967bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1225 - accuracy: 0.9333\n",
            "Accuracy: 93.33%\n"
          ]
        }
      ]
    }
  ]
}